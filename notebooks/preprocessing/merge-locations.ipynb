{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"merge-locations.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNK5xw5TbHk/or0NVKqsZVG"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"kQtIbvWVxkM0","cellView":"form"},"source":["#@markdown <b>Run me to import underscore module</b><br/>   {display-mode: \"form\"}\n","#@markdown <small>Method signatures:</small><br/> \n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _(source_path, target_path)</small></small><br/>\n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _set_gh_token(token)</small></small><br/>\n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _from_gh(user_name, repo_name, release_name) &nbsp; &nbsp; &nbsp; <b>Returns:</B> dictionary of arrays { 'array_name' : np.ndarray }</small></small><br/>\n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _to_gh(user_name, repo_name, release_name, split_size=600, **arr_kwargs)</small></small><br/>\n","\n","!pip install -q githubrelease\n","import numpy as np\n","import os, glob, re, time\n","import github_release\n","\n","compressed_dirs = set()\n","\n","\n","def _compress(source_path, target_path, target_dir=None):\n","    if target_dir:\n","        !mkdir -p {target_dir}\n","    if target_path.endswith('.tar.gz'):\n","        !tar -czf {target_path} -C {source_path} .\n","    elif target_path.endswith('.tar'):\n","        !tar -cf {target_path} -C {source_path} .\n","    elif target_path.endswith('.zip'):\n","        !(cd {source_path} && zip -q -r {target_path} .)\n","\n","\n","def _extract(source_path, target_path):\n","    !mkdir -p {target_path}\n","    if source_path.endswith('.tar.gz'):\n","        !tar -xzf {source_path} -C {target_path}\n","    elif source_path.endswith('.tar'):\n","        !tar -xf {source_path} -C {target_path}\n","    elif source_path.endswith('.zip'):\n","        !unzip -qq {source_path} -d {target_path}\n","\n","\n","def _(source_path, target_path):\n","    \"\"\"\n","    Use cases:\n","        Movement:\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","            \n","        Compression (e.g. from dir to .tar.gz):\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","            \n","        Extraction (e.g. from .zip to dir):\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","            \n","        Extraction & compression (e.g. from .zip to .tar.gz):\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","    \"\"\"\n","    COMPRESSION_FORMATS = ('zip', 'tar', 'tar.gz')\n","    TEMP_DIR = \"/tmp_\"\n","    LOG_TEMPLATE = \"{}    from    {}    to    {}\"\n","\n","    # Source\n","    source_dir, _, source_name = source_path.rpartition('/')\n","    source_isgcs = source_path.startswith(\"gs://\")\n","    source_islocal = not source_isgcs\n","    source_isprefix, source_isfile, source_ext = source_name.partition('.')\n","    source_isdir = not source_isfile\n","    source_iscompression = source_ext in COMPRESSION_FORMATS\n","\n","    # Target\n","    target_dir, _, target_name = target_path.rpartition('/')\n","    target_isgcs = target_path.startswith(\"gs://\")\n","    target_islocal = not target_isgcs\n","    target_prefix, target_isfile, target_ext = target_name.partition('.')\n","    target_isdir = not target_isfile\n","    target_iscompression = target_ext in COMPRESSION_FORMATS\n","\n","    # Flags\n","    MOVE_ONLY = source_ext == target_ext\n","    GCS_ONLY = source_isgcs and target_isgcs\n","    RENAME = source_isprefix != target_prefix\n","    COMPRESSION = source_isdir and target_iscompression\n","    EXTRACTION = source_iscompression and target_isdir\n","    EXTRACTION_COMPRESSION = source_iscompression and target_iscompression and source_ext != target_ext\n","\n","    # Authenticate if writing to GCS\n","    if target_isgcs:\n","        from google.colab import auth\n","        auth.authenticate_user()\n","\n","    # Assert that subdirectories exist if target is local\n","    if target_islocal:\n","        !mkdir -p {target_dir}\n","\n","    # Movement commands\n","    if MOVE_ONLY:\n","        # GCS -> GCS\n","        if source_isgcs and target_isgcs:\n","            print(LOG_TEMPLATE.format(\"MOVING (1/1)\", source_path, target_path))\n","            !gsutil -m -q mv {source_path} {target_path}\n","        \n","        # LOCAL -> LOCAL\n","        elif source_islocal and target_islocal:\n","            print(LOG_TEMPLATE.format(\"MOVING (1/1)\", source_path, target_path))\n","            !mv {source_path} {target_path}\n","        \n","        # GCS -> LOCAL\n","        elif source_isgcs and target_islocal:\n","            if source_isdir:\n","                print(LOG_TEMPLATE.format(\"DOWNLOADING DIR (1/1)\", source_path, target_dir))\n","                !gsutil -m -q cp -r {source_path} {target_dir}\n","                if RENAME:\n","                    print(LOG_TEMPLATE.format(\"\\tRENAMING DIR\", source_isprefix, target_prefix))\n","                    !mv {target_dir}/{source_isprefix} {target_dir}/{target_prefix}\n","            else:\n","                print(LOG_TEMPLATE.format(\"DOWNLOADING FILE (1/1)\", source_path, target_path))\n","                !gsutil -m -q cp {source_path} {target_path}\n","        \n","        # LOCAL -> GCS\n","        if source_islocal and target_isgcs:\n","            if source_isdir:\n","                print(LOG_TEMPLATE.format(\"UPLOADING DIR (1/1)\", source_path, target_path))\n","                !gsutil -m -q cp -r {source_path} {target_path}\n","            else:\n","                print(LOG_TEMPLATE.format(\"UPLOADING FILE (1/1)\", source_path, target_path))\n","                !gsutil -m -q cp {source_path} {target_path}\n","        return\n","\n","\n","    # Create directory for intermediate storage if required\n","    if source_isgcs or target_isgcs or EXTRACTION_COMPRESSION:\n","        !mkdir -p {TEMP_DIR}\n","    \n","\n","    # For remaining operations, download GCS source to temp and treat as local\n","    if source_isgcs:\n","        if source_isdir:\n","            print(LOG_TEMPLATE.format(\"\\tDOWNLOADING DIR\", source_path, TEMP_DIR))\n","            !gsutil -m -q cp -r {source_path} {TEMP_DIR}\n","        else:\n","            print(LOG_TEMPLATE.format(\"\\tDOWNLOADING FILE\", source_path, f\"{TEMP_DIR}/{source_name}\"))\n","            !gsutil -m -q cp {source_path} {TEMP_DIR}/{source_name}\n","        source_path = f\"{TEMP_DIR}/{source_name}\"\n","        source_dir = TEMP_DIR\n","\n","    # Compression\n","    if COMPRESSION:\n","        if target_islocal:\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (1/1)\", source_path, target_path))\n","            _compress(source_path, target_path, target_dir=target_dir)\n","        else:\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (1/2)\", source_path, f\"{TEMP_DIR}/{target_name}\"))\n","            _compress(source_path, f\"{TEMP_DIR}/{target_name}\")\n","            print(LOG_TEMPLATE.format(\"UPLOADING FILE (2/2)\", f\"{TEMP_DIR}/{target_name}\", target_path))\n","            !gsutil -m -q cp {TEMP_DIR}/{target_name} {target_path}\n","\n","    # Extraction\n","    elif EXTRACTION:\n","        if target_islocal:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/1)\", source_path, target_path))\n","            _extract(source_path, target_path)\n","        else:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/2)\", source_path, f\"{TEMP_DIR}/{target_name}\"))\n","            _extract(source_path, f\"{TEMP_DIR}/{target_name}\")\n","            print(LOG_TEMPLATE.format(\"UPLOADING DIR (2/2)\", f\"{TEMP_DIR}/{target_name}\", target_path))\n","            !gsutil -m -q cp -r {TEMP_DIR}/{target_name} {target_path}\n","\n","    # Extraction & compression\n","    elif EXTRACTION_COMPRESSION:\n","        if target_islocal:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/2)\", source_path, f\"{TEMP_DIR}/{target_prefix}\"))\n","            _extract(source_path, f\"{TEMP_DIR}/{target_prefix}\")\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (2/2)\", f\"{TEMP_DIR}/{target_prefix}\", target_path))\n","            _compress(f\"{TEMP_DIR}/{target_prefix}\", target_path, target_dir=target_dir)\n","        else:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/3)\", source_path, f\"{TEMP_DIR}/{target_prefix}\"))\n","            _extract(source_path, f\"{TEMP_DIR}/{target_prefix}\")\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (2/3)\", f\"{TEMP_DIR}/{target_prefix}\", f\"{TEMP_DIR}/{target_name}\"))\n","            _compress(f\"{TEMP_DIR}/{target_prefix}\", f\"{TEMP_DIR}/{target_name}\")\n","            print(LOG_TEMPLATE.format(\"UPLOADING FILE (3/3)\", f\"{TEMP_DIR}/{target_name}\", target_path))\n","            !gsutil -m -q cp {TEMP_DIR}/{target_name} {target_path}\n","    \n","    # Cleanup intermediate storage\n","    !rm -rf {TEMP_DIR}\n","\n","\n","def _set_gh_token(token):\n","    os.environ[\"GITHUB_TOKEN\"] = token\n","\n","\n","def _export_array(array, release_name, prefix=\"\", splits=3):\n","    dir_path = f\"/tmp_/{release_name}\"\n","    !mkdir -p {dir_path}\n","    n_digits = len(str(splits - 1))\n","    subarrays = np.array_split(array, splits)\n","    for i, subarray in enumerate(subarrays):\n","        filename = f\"{prefix}__{str(i).zfill(n_digits)}.npy\"\n","        np.save(f\"{dir_path}/{filename}\", subarray)\n","\n","\n","def _concat_arrays(paths):\n","    return np.concatenate([np.load(path, allow_pickle=True) for path in sorted(paths)])\n","\n","\n","def _to_gh(user_name, repo_name, release_name, split_size=600, **arr_kwargs):\n","    # Assert that GitHub Auth token is set\n","    if \"GITHUB_TOKEN\" not in os.environ:\n","        print(\"GitHub authentication token is not set.\")\n","        print(\"Set token using the '_set_gh_token(token_string)' method.\")\n","        print(\"Minimal required auth scope is 'repo/public_repo' for public repositories.\")\n","        print(\"URL: https://github.com/settings/tokens/new\")\n","        return\n","\n","    # Split arrays\n","    for prefix, array in arr_kwargs.items():\n","        splits = int((array.nbytes/1_000_000) // split_size) + 1\n","        _export_array(array, release_name, prefix=prefix, splits=splits)\n","\n","    # Upload arrays\n","    github_release.gh_release_create(\n","        f\"{user_name}/{repo_name}\", \n","        release_name, \n","        publish=True, \n","        name=release_name, \n","        asset_pattern=f\"/tmp_/{release_name}/*\"\n","    )\n","    !rm -rf /tmp_/*\n","\n","\n","def _from_gh(user_name, repo_name, release_name):\n","    # Download release to temporary directory\n","    print(\"Downloading dataset in parallell ... \", end='\\t')\n","    t0 = time.perf_counter()\n","    assets = github_release.get_assets(f\"{user_name}/{repo_name}\", tag_name=release_name)\n","    download_urls = [asset['browser_download_url'] for asset in assets]\n","    urls_str = \" \".join(download_urls)\n","    !echo {urls_str} | xargs -n 1 -P 8 wget -q -P /tmp_/{release_name}_dl/\n","    t1 = time.perf_counter()\n","    print(f\"done! ({t1 - t0:.3f} seconds)\")\n","\n","    # Load data into numpy arrays\n","    paths = glob.glob(f\"/tmp_/{release_name}_dl/*.npy\")\n","    groups = {}\n","    for path in paths:\n","        match = re.match(r\".*/(.*)__[0-9]*\\.npy\", path)\n","        if match:\n","            prefix = match.group(1)\n","            groups[prefix] = groups.get(prefix, []) + [path]\n","    arrays_dict = {name: _concat_arrays(paths) for name, paths in groups.items()}\n","    !rm -rf /tmp_/*\n","    return arrays_dict\n","    \n","\n","def _log_to_gh(user, repo, tag, log_dir=\"/tmp/logs\"):\n","    # Create temporary directory for compressed logs\n","    !mkdir -p /tmp/compressed_logs\n","    \n","    # Compress all directories in log dir\n","    for dirname in os.listdir(log_dir):\n","        # Skip files\n","        if \".\" in dirname or dirname in compressed_dirs:\n","            continue\n","\n","        # Compress\n","        _(f\"{log_dir}/{dirname}\", f\"/tmp/compressed_logs/{dirname}.tar.gz\")\n","        compressed_dirs.add(dirname)\n","\n","    # Upload compressed logs to GitHub\n","    github_release.gh_asset_upload(f\"{user}/{repo}\", tag, f\"/tmp/compressed_logs/*.tar.gz\")\n","\n","    # Cleanup compressed logs\n","    !rm -rf /tmp/compressed_logs/*"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_COh2J_5JmwT"},"source":["#### Load dataset into DataFrames from .parquet files\n"]},{"cell_type":"code","metadata":{"id":"b0P_csYGJmwV"},"source":["import pandas as pd\n","\n","# Download .parquet files\n","SOURCE_PATH = \"gs://telenor-data-science/datasets/location_dataset_parquet\"\n","TARGET_PATH = \"/content/location_dataset_parquet\"\n","_(SOURCE_PATH, TARGET_PATH)\n","\n","# Load all .parquet files as dataframes\n","dataframes = {}     # Format: {location: pd.DataFrame}\n","for path in glob.glob(f\"{TARGET_PATH}/**/*.parquet\", recursive=True):\n","    df = pd.read_parquet(path)\n","    location = path.split(os.sep)[-1].split('.')[0]\n","    dataframes[location] = df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FA32XFu1ZZt2"},"source":["# Create column groupings\n","patterns = [\"MET\", \"KV\", \"PRA\"]\n","patterns = [\n","    (\"PRA_(\\d+)__(upTo5_6)\", \"PRA__upTo5_6\"),\n","    (\"PRA_(\\d+)__(from5_6To7_6)\", \"PRA__from5_6To7_6\"),\n","    (\"PRA_(\\d+)__(from7_6To12_5)\", \"PRA__from7_6To12_5\"),\n","    (\"PRA_(\\d+)__(from12_5To16)\", \"PRA__from12_5To16\"),\n","    (\"PRA_(\\d+)__(from16To24)\", \"PRA__from16To24\"),\n","    (\"PRA_(\\d+)__(from24up)\", \"PRA__from24up\"),\n","]\n","\n","merged_df = pd.DataFrame(columns=['location', 'year'])\n","\n","# Aggregate PRA columns for each locatioin\n","for location, df in dataframes.items():\n","    for pattern, target_column in patterns:\n","        col_group = [col for col in df.columns if re.match(pattern, col)]\n","        grouped_df = df[col_group]\n","\n","        # Min-max normalization\n","        series_min = grouped_df.min(axis=0, skipna=True)\n","        series_max = grouped_df.max(axis=0, skipna=True)\n","        df_scaled = grouped_df.subtract(series_min, axis=1).divide(series_max, axis=1)\n","\n","        # Calculate aggregated column\n","        df_scaled[target_column] = df_scaled.mean(axis=1, skipna=True)\n","        \n","        # Remove grouped columns from original dataframe and append new column\n","        df = df.drop(col_group, axis=1)\n","        df = pd.concat([df, df_scaled[target_column]], axis=1)\n","    \n","    # Generate non-existing columns in merged dataframe\n","    for col_name in set(df.columns) - set(merged_df.columns):\n","        merged_df[col_name] = None\n","\n","    # Add processed location dataframe to merged dataframe\n","    merged_df = merged_df.append(df)\n","merged_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BXxpNmG57Xb1"},"source":["col_order = [\n","    \"location\",\n","    \"year\",\n","    \"KV_moving_0m_to_100m\",\n","    \"KV_moving_100m_to_300m\",\n","    \"KV_moving_300m_to_1000m\",\n","    \"KV_moving_1000m_to_3000m\",\n","    \"KV_moving_3000m_to_10000m\",\n","    \"KV_moving_10000m_to_30000m\",\n","    \"KV_stationary_0m_to_100m\",\n","    \"KV_stationary_100m_to_300m\",\n","    \"KV_stationary_300m_to_1000m\",\n","    \"KV_stationary_1000m_to_3000m\",\n","    \"KV_stationary_3000m_to_10000m\",\n","    \"KV_stationary_10000m_to_30000m\",\n","    \"MET_air_temperature_2m\",\n","    \"MET_air_temperature_10m\",\n","    \"MET_air_temperature_25m\",\n","    \"MET_air_temperature_30m\",\n","    \"MET_cloud_area_fraction\",\n","    \"MET_relative_humidity\",\n","    \"MET_sea_surface_temperature\",\n","    \"MET_surface_air_pressure\",\n","    \"MET_surface_snow_thickness\",\n","    \"MET_wind_from_direction_2m\",\n","    \"MET_wind_from_direction_10m\",\n","    \"MET_wind_speed_2m\",\n","    \"MET_wind_speed_10m\",\n","    \"NEA_NO\",\n","    \"NEA_NO2\",\n","    \"NEA_NOx\",\n","    \"NEA_PM1\",\n","    \"NEA_PM2_5\",\n","    \"NEA_PM10\",\n","    \"PRA__upTo5_6\",\n","    \"PRA__from5_6To7_6\",\n","    \"PRA__from7_6To12_5\",\n","    \"PRA__from12_5To16\",\n","    \"PRA__from16To24\",\n","    \"PRA__from24up\",\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"auPJBu2i59gO"},"source":["merged_df = merged_df.reindex(sorted(merged_df.columns, key=lambda col: col_order.index(col)), axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qno6wToR8yHL"},"source":["merged_df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qbHJ_4Lq5wiC"},"source":["merged_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VLhHMYH17PFn"},"source":["def export_to_parquet(dataframes, target_dir):\n","    if target_dir.endswith(\"/\"):\n","        target_dir = target_dir[:-1]\n","    !mkdir -p {target_dir}\n","\n","    # Export dataframes to PARQUET\n","    for location, df in dataframes.items():\n","        df.to_parquet(f\"{target_dir}/{location}.parquet\")\n","\n","def bigquery_dataset_from_dataframes(dataframes, dataset_name):\n","    PROJECT_ID = 'telenor-data-science'\n","    TARGET_GCS_DIR =  f\"gs://{PROJECT_ID}/datasets/{dataset_name}\"\n","    \n","    # Export to local .parquet files\n","    export_to_parquet(dataframes, f\"/tmp_/{dataset_name}\")\n","\n","    # Upload dataset to GCS\n","    _(f\"/tmp_/{dataset_name}\", TARGET_GCS_DIR)\n","\n","    # Create new BigQuery dataset\n","    from google.colab import auth\n","    auth.authenticate_user()\n","    !gcloud config set project {PROJECT_ID}\n","    !bq mk --location europe-west1 --dataset {dataset_name}\n","\n","    # Create new table for every parquet file\n","    for table_name in dataframes.keys():\n","        !bq load  --source_format=PARQUET --autodetect {dataset_name}.{table_name} {TARGET_GCS_DIR}/{table_name}.parquet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GMieuQk889wp"},"source":["merged_dataframes = {\"all_locations\": merged_df}\n","bigquery_dataset_from_dataframes(merged_dataframes, \"merged_dataset\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eiwGz_mmuqx7"},"source":["\n","        col_groups = {provider: [col for col in df.columns if (col.startswith(provider) or col == 'year')]  for pattern, target in providers}\n","\n","    # For each station\n","    for provider, col_group in col_groups.items():\n","        year_dfs = {x: pd.DataFrame(y) for x, y in df[col_group].groupby('year', as_index=False)}\n","\n","for location, df in dataframes.items():\n","    # Group columns by provider\n","    col_groups = {provider: [col for col in df.columns if (col.startswith(provider) or col == 'year')]  for provider in providers}\n","\n","    # For each provider\n","    for provider, col_group in col_groups.items():\n","        year_dfs = {x: pd.DataFrame(y) for x, y in df[col_group].groupby('year', as_index=False)}\n","\n","        # For each year\n","        years_missing = {'location': location}      # Format: {year: missing_ratio]\n","        for year, year_df in year_dfs.items():\n","            year_df = year_df.drop('year', axis=1)\n","            missing_ratio = year_df.isnull().astype(int).sum().sum() / year_df.size\n","            #print(f\"Year: {year},   Missing ratio: {missing_ratio}\")\n","            years_missing[year] = [1 - missing_ratio]\n","        \n","        # Add this locations' missing ratios to provider \n","        years_missing_df = pd.DataFrame(years_missing)\n","        provider_df = provider_dfs[provider]\n","        \n","        # Generate colummns in provider dataframe\n","        for column_name in set(years_missing_df.columns) - set(provider_df.columns):\n","            provider_df[column_name] = None\n","\n","        # Add year/location data to provider\n","        provider_dfs[provider] = provider_df.append(years_missing_df)\n","\n","# Rearrange columns\n","provider_dfs = {provider: df.reindex(sorted(df.columns), axis=1).fillna(0).sort_values('location') for provider, df in provider_dfs.items()}\n","\n","# Create concatenated dataframe for stacked chart\n","concat_dfs = []\n","for provider, df in provider_dfs.items():\n","    df['provider'] = provider\n","    concat_dfs.append(df)\n","concat_df = pd.concat(concat_dfs, ignore_index=True)\n","concat_df = concat_df.set_index(['location', 'provider'])\n","concat_df = concat_df.sort_values(['location', 'provider'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KMrHFt9oXW2p"},"source":["merged_df.to_csv(\"all_locations.csv\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"63nKBx3vXjZe"},"source":["_(\"/content/all_locations.csv\", \"gs://telenor-data-science/datasets/merged_dataset/all_locations.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TqcrhaFMXXYT"},"source":[""],"execution_count":null,"outputs":[]}]}