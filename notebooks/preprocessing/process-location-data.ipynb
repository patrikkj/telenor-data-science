{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"process-location-data.ipynb","provenance":[{"file_id":"1Hm6nPMV05oWxt5p3i6LJx2jafQ3I_SBy","timestamp":1604317416001}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"kQtIbvWVxkM0","cellView":"form","executionInfo":{"status":"ok","timestamp":1604317576043,"user_tz":-60,"elapsed":3554,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"17621156606987172084"}}},"source":["#@markdown <b>Run me to import underscore module</b><br/>   {display-mode: \"form\"}\n","#@markdown <small>Method signatures:</small><br/> \n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _(source_path, target_path)</small></small><br/>\n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _set_gh_token(token)</small></small><br/>\n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _from_gh(user_name, repo_name, release_name) &nbsp; &nbsp; &nbsp; <b>Returns:</B> dictionary of arrays { 'array_name' : np.ndarray }</small></small><br/>\n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _to_gh(user_name, repo_name, release_name, split_size=600, **arr_kwargs)</small></small><br/>\n","\n","!pip install -q githubrelease\n","import numpy as np\n","import os, glob, re, time\n","import github_release\n","\n","compressed_dirs = set()\n","\n","\n","def _compress(source_path, target_path, target_dir=None):\n","    if target_dir:\n","        !mkdir -p {target_dir}\n","    if target_path.endswith('.tar.gz'):\n","        !tar -czf {target_path} -C {source_path} .\n","    elif target_path.endswith('.tar'):\n","        !tar -cf {target_path} -C {source_path} .\n","    elif target_path.endswith('.zip'):\n","        !(cd {source_path} && zip -q -r {target_path} .)\n","\n","\n","def _extract(source_path, target_path):\n","    !mkdir -p {target_path}\n","    if source_path.endswith('.tar.gz'):\n","        !tar -xzf {source_path} -C {target_path}\n","    elif source_path.endswith('.tar'):\n","        !tar -xf {source_path} -C {target_path}\n","    elif source_path.endswith('.zip'):\n","        !unzip -qq {source_path} -d {target_path}\n","\n","\n","def _(source_path, target_path):\n","    \"\"\"\n","    Use cases:\n","        Movement:\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","            \n","        Compression (e.g. from dir to .tar.gz):\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","            \n","        Extraction (e.g. from .zip to dir):\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","            \n","        Extraction & compression (e.g. from .zip to .tar.gz):\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","    \"\"\"\n","    COMPRESSION_FORMATS = ('zip', 'tar', 'tar.gz')\n","    TEMP_DIR = \"/tmp_\"\n","    LOG_TEMPLATE = \"{}    from    {}    to    {}\"\n","\n","    # Source\n","    source_dir, _, source_name = source_path.rpartition('/')\n","    source_isgcs = source_path.startswith(\"gs://\")\n","    source_islocal = not source_isgcs\n","    source_isprefix, source_isfile, source_ext = source_name.partition('.')\n","    source_isdir = not source_isfile\n","    source_iscompression = source_ext in COMPRESSION_FORMATS\n","\n","    # Target\n","    target_dir, _, target_name = target_path.rpartition('/')\n","    target_isgcs = target_path.startswith(\"gs://\")\n","    target_islocal = not target_isgcs\n","    target_prefix, target_isfile, target_ext = target_name.partition('.')\n","    target_isdir = not target_isfile\n","    target_iscompression = target_ext in COMPRESSION_FORMATS\n","\n","    # Flags\n","    MOVE_ONLY = source_ext == target_ext\n","    GCS_ONLY = source_isgcs and target_isgcs\n","    RENAME = source_isprefix != target_prefix\n","    COMPRESSION = source_isdir and target_iscompression\n","    EXTRACTION = source_iscompression and target_isdir\n","    EXTRACTION_COMPRESSION = source_iscompression and target_iscompression and source_ext != target_ext\n","\n","    # Authenticate if writing to GCS\n","    if target_isgcs:\n","        from google.colab import auth\n","        auth.authenticate_user()\n","\n","    # Assert that subdirectories exist if target is local\n","    if target_islocal:\n","        !mkdir -p {target_dir}\n","\n","    # Movement commands\n","    if MOVE_ONLY:\n","        # GCS -> GCS\n","        if source_isgcs and target_isgcs:\n","            print(LOG_TEMPLATE.format(\"MOVING (1/1)\", source_path, target_path))\n","            !gsutil -m -q mv {source_path} {target_path}\n","        \n","        # LOCAL -> LOCAL\n","        elif source_islocal and target_islocal:\n","            print(LOG_TEMPLATE.format(\"MOVING (1/1)\", source_path, target_path))\n","            !mv {source_path} {target_path}\n","        \n","        # GCS -> LOCAL\n","        elif source_isgcs and target_islocal:\n","            if source_isdir:\n","                print(LOG_TEMPLATE.format(\"DOWNLOADING DIR (1/1)\", source_path, target_dir))\n","                !gsutil -m -q cp -r {source_path} {target_dir}\n","                if RENAME:\n","                    print(LOG_TEMPLATE.format(\"\\tRENAMING DIR\", source_isprefix, target_prefix))\n","                    !mv {target_dir}/{source_isprefix} {target_dir}/{target_prefix}\n","            else:\n","                print(LOG_TEMPLATE.format(\"DOWNLOADING FILE (1/1)\", source_path, target_path))\n","                !gsutil -m -q cp {source_path} {target_path}\n","        \n","        # LOCAL -> GCS\n","        if source_islocal and target_isgcs:\n","            if source_isdir:\n","                print(LOG_TEMPLATE.format(\"UPLOADING DIR (1/1)\", source_path, target_path))\n","                !gsutil -m -q cp -r {source_path} {target_path}\n","            else:\n","                print(LOG_TEMPLATE.format(\"UPLOADING FILE (1/1)\", source_path, target_path))\n","                !gsutil -m -q cp {source_path} {target_path}\n","        return\n","\n","\n","    # Create directory for intermediate storage if required\n","    if source_isgcs or target_isgcs or EXTRACTION_COMPRESSION:\n","        !mkdir -p {TEMP_DIR}\n","    \n","\n","    # For remaining operations, download GCS source to temp and treat as local\n","    if source_isgcs:\n","        if source_isdir:\n","            print(LOG_TEMPLATE.format(\"\\tDOWNLOADING DIR\", source_path, TEMP_DIR))\n","            !gsutil -m -q cp -r {source_path} {TEMP_DIR}\n","        else:\n","            print(LOG_TEMPLATE.format(\"\\tDOWNLOADING FILE\", source_path, f\"{TEMP_DIR}/{source_name}\"))\n","            !gsutil -m -q cp {source_path} {TEMP_DIR}/{source_name}\n","        source_path = f\"{TEMP_DIR}/{source_name}\"\n","        source_dir = TEMP_DIR\n","\n","    # Compression\n","    if COMPRESSION:\n","        if target_islocal:\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (1/1)\", source_path, target_path))\n","            _compress(source_path, target_path, target_dir=target_dir)\n","        else:\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (1/2)\", source_path, f\"{TEMP_DIR}/{target_name}\"))\n","            _compress(source_path, f\"{TEMP_DIR}/{target_name}\")\n","            print(LOG_TEMPLATE.format(\"UPLOADING FILE (2/2)\", f\"{TEMP_DIR}/{target_name}\", target_path))\n","            !gsutil -m -q cp {TEMP_DIR}/{target_name} {target_path}\n","\n","    # Extraction\n","    elif EXTRACTION:\n","        if target_islocal:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/1)\", source_path, target_path))\n","            _extract(source_path, target_path)\n","        else:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/2)\", source_path, f\"{TEMP_DIR}/{target_name}\"))\n","            _extract(source_path, f\"{TEMP_DIR}/{target_name}\")\n","            print(LOG_TEMPLATE.format(\"UPLOADING DIR (2/2)\", f\"{TEMP_DIR}/{target_name}\", target_path))\n","            !gsutil -m -q cp -r {TEMP_DIR}/{target_name} {target_path}\n","\n","    # Extraction & compression\n","    elif EXTRACTION_COMPRESSION:\n","        if target_islocal:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/2)\", source_path, f\"{TEMP_DIR}/{target_prefix}\"))\n","            _extract(source_path, f\"{TEMP_DIR}/{target_prefix}\")\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (2/2)\", f\"{TEMP_DIR}/{target_prefix}\", target_path))\n","            _compress(f\"{TEMP_DIR}/{target_prefix}\", target_path, target_dir=target_dir)\n","        else:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/3)\", source_path, f\"{TEMP_DIR}/{target_prefix}\"))\n","            _extract(source_path, f\"{TEMP_DIR}/{target_prefix}\")\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (2/3)\", f\"{TEMP_DIR}/{target_prefix}\", f\"{TEMP_DIR}/{target_name}\"))\n","            _compress(f\"{TEMP_DIR}/{target_prefix}\", f\"{TEMP_DIR}/{target_name}\")\n","            print(LOG_TEMPLATE.format(\"UPLOADING FILE (3/3)\", f\"{TEMP_DIR}/{target_name}\", target_path))\n","            !gsutil -m -q cp {TEMP_DIR}/{target_name} {target_path}\n","    \n","    # Cleanup intermediate storage\n","    !rm -rf {TEMP_DIR}\n","\n","\n","def _set_gh_token(token):\n","    os.environ[\"GITHUB_TOKEN\"] = token\n","\n","\n","def _export_array(array, release_name, prefix=\"\", splits=3):\n","    dir_path = f\"/tmp_/{release_name}\"\n","    !mkdir -p {dir_path}\n","    n_digits = len(str(splits - 1))\n","    subarrays = np.array_split(array, splits)\n","    for i, subarray in enumerate(subarrays):\n","        filename = f\"{prefix}__{str(i).zfill(n_digits)}.npy\"\n","        np.save(f\"{dir_path}/{filename}\", subarray)\n","\n","\n","def _concat_arrays(paths):\n","    return np.concatenate([np.load(path, allow_pickle=True) for path in sorted(paths)])\n","\n","\n","def _to_gh(user_name, repo_name, release_name, split_size=600, **arr_kwargs):\n","    # Assert that GitHub Auth token is set\n","    if \"GITHUB_TOKEN\" not in os.environ:\n","        print(\"GitHub authentication token is not set.\")\n","        print(\"Set token using the '_set_gh_token(token_string)' method.\")\n","        print(\"Minimal required auth scope is 'repo/public_repo' for public repositories.\")\n","        print(\"URL: https://github.com/settings/tokens/new\")\n","        return\n","\n","    # Split arrays\n","    for prefix, array in arr_kwargs.items():\n","        splits = int((array.nbytes/1_000_000) // split_size) + 1\n","        _export_array(array, release_name, prefix=prefix, splits=splits)\n","\n","    # Upload arrays\n","    github_release.gh_release_create(\n","        f\"{user_name}/{repo_name}\", \n","        release_name, \n","        publish=True, \n","        name=release_name, \n","        asset_pattern=f\"/tmp_/{release_name}/*\"\n","    )\n","    !rm -rf /tmp_/*\n","\n","\n","def _from_gh(user_name, repo_name, release_name):\n","    # Download release to temporary directory\n","    print(\"Downloading dataset in parallell ... \", end='\\t')\n","    t0 = time.perf_counter()\n","    assets = github_release.get_assets(f\"{user_name}/{repo_name}\", tag_name=release_name)\n","    download_urls = [asset['browser_download_url'] for asset in assets]\n","    urls_str = \" \".join(download_urls)\n","    !echo {urls_str} | xargs -n 1 -P 8 wget -q -P /tmp_/{release_name}_dl/\n","    t1 = time.perf_counter()\n","    print(f\"done! ({t1 - t0:.3f} seconds)\")\n","\n","    # Load data into numpy arrays\n","    paths = glob.glob(f\"/tmp_/{release_name}_dl/*.npy\")\n","    groups = {}\n","    for path in paths:\n","        match = re.match(r\".*/(.*)__[0-9]*\\.npy\", path)\n","        if match:\n","            prefix = match.group(1)\n","            groups[prefix] = groups.get(prefix, []) + [path]\n","    arrays_dict = {name: _concat_arrays(paths) for name, paths in groups.items()}\n","    !rm -rf /tmp_/*\n","    return arrays_dict\n","    \n","\n","def _log_to_gh(user, repo, tag, log_dir=\"/tmp/logs\"):\n","    # Create temporary directory for compressed logs\n","    !mkdir -p /tmp/compressed_logs\n","    \n","    # Compress all directories in log dir\n","    for dirname in os.listdir(log_dir):\n","        # Skip files\n","        if \".\" in dirname or dirname in compressed_dirs:\n","            continue\n","\n","        # Compress\n","        _(f\"{log_dir}/{dirname}\", f\"/tmp/compressed_logs/{dirname}.tar.gz\")\n","        compressed_dirs.add(dirname)\n","\n","    # Upload compressed logs to GitHub\n","    github_release.gh_asset_upload(f\"{user}/{repo}\", tag, f\"/tmp/compressed_logs/*.tar.gz\")\n","\n","    # Cleanup compressed logs\n","    !rm -rf /tmp/compressed_logs/*"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XzSuSCgdVPaD"},"source":["### Link to Google Drive: https://drive.google.com/drive/folders/1Pnuo1tB1XtiDjMa7eXmuUctL2XX9emU7\n","### Link to Tableau Online: https://dub01.online.tableau.com/#/site/telenordashboard/projects/123532\n","### Link to Google Cloud Storage: https://console.cloud.google.com/storage/browser?project=telenor-data-science\n","### Link to Google Cloud BigQuery: https://console.cloud.google.com/bigquery?project=telenor-data-science&p=telenor-data-science&d=telenor_dataset&page=dataset"]},{"cell_type":"code","metadata":{"id":"MSnRVwRihC_N","executionInfo":{"status":"ok","timestamp":1604317576044,"user_tz":-60,"elapsed":1305,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"17621156606987172084"}}},"source":["import glob\n","import os\n","import pandas as pd"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"rfOfPrlAWflD","executionInfo":{"status":"ok","timestamp":1604317608116,"user_tz":-60,"elapsed":575,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"17621156606987172084"}}},"source":["# Project info\n","PROJECT_ID = 'telenor-data-science'\n","DATASET = 'location_dataset.tar.gz'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"JsYusB9hgzpG","cellView":"code","executionInfo":{"status":"ok","timestamp":1604317620297,"user_tz":-60,"elapsed":8293,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"17621156606987172084"}}},"source":["# Download dataset from GCS\n","!gsutil -q cp gs://{PROJECT_ID}/datasets/{DATASET} /tmp\n","!mkdir -p data\n","!tar -zxf /tmp/{DATASET} -C /content/data"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"SJ7uL483jaAE","executionInfo":{"status":"ok","timestamp":1604320552392,"user_tz":-60,"elapsed":3993,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"17621156606987172084"}},"outputId":"ca216678-b529-40e5-d9d5-51ec1d5b75ef","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["paths = glob.glob(\"/content/data/**/*.parquet\", recursive=True)\n","dataframes = {}\n","summary_dfs = []\n","\n","prefixes = [\"MET\", \"KV\", \"PRA\", \"NEA\"]\n","\n","# Load all parquet files as dataframes\n","for path in paths:\n","    dir_name, sep, file_name = path.rpartition(os.sep)\n","    location = file_name.split('.')[0]\n","    location_df = pd.read_parquet(path)\n","    #print(f\"Location: {location}\")\n","    \n","    location_data = []\n","    col_groups = {prefix: [col for col in df.columns if col.startswith(prefix)] for prefix in prefixes}\n","    for prefix, col_group in col_groups.items():\n","        location_data.append(df[col_group].isnull().sum().sum()/df[col_group].size)\n","    \n","    summary_dfs.append([location] + location_data)\n","\n","    #print(col_groups)\n","    \n","    \n","    #print(\"\\n\"*3)\n","    #df.isna().describe()\n","\"\"\"\n","    df_tuple = (df, os.path.splitext(path)[0][-4:])\n","    df_old = dataframes.get(dir_name, [])\n","    df_old.append(df_tuple)\n","    dataframes[dir_name] = df_old\n","\n","## Create mapping {location -> [[df1, df2, df3, ...], [year1, year2, year3, ...]]}\n","##for k, v in dataframes.items():\n","#    dataframes[k] = list(zip(*v))\n","\n","#df2 = pd.concat([df.drop(['location', 'year'], axis=1).isnull().sum(), df.describe().transpose()], axis=1)\n","#df2.hist(column=0, bins=20)\n","\"\"\";\n","pd.DataFrame(summary_dfs)"],"execution_count":69,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in double_scalars\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>bryn_skole</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>klosterhaugen</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>kransen</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>vangsveien__hamar</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>raÃädal</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>kannik</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>alnabru</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>moheia_vest</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>elgeseter</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>e6_tiller</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>hansjordnesbukta</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>solheim</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>furulund</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>rv_4__aker_sykehus</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>torvet</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>hjortnes</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>lensmannsdalen</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>alvim</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>schancheholen</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>bankplassen</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>bakke_kirke</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>ringsakervegen</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>eilif_dues_vei</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>lillehammer_barnehage</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>bekkestua</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>st_croix</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>rolland__aÃäsane</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>minnesundvegen__gj√∏vik</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>vigernes</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>nedre_langgate</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>kirkeveien</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>loddefjord</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>seljestad_rv83</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>danmarks_plass</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>vaÃäland</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>leiret</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>sofienbergparken</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>smestad</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>bygd√∏y_alle</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>e6_alna_senter</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>knarrdalstranda</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>nygaardsgata</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>manglerud</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>olav_v_gate</td>\n","      <td>0.011385</td>\n","      <td>NaN</td>\n","      <td>0.138893</td>\n","      <td>0.507519</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         0         1   2         3         4\n","0               bryn_skole  0.011385 NaN  0.138893  0.507519\n","1            klosterhaugen  0.011385 NaN  0.138893  0.507519\n","2                  kransen  0.011385 NaN  0.138893  0.507519\n","3        vangsveien__hamar  0.011385 NaN  0.138893  0.507519\n","4                   raÃädal  0.011385 NaN  0.138893  0.507519\n","5                   kannik  0.011385 NaN  0.138893  0.507519\n","6                  alnabru  0.011385 NaN  0.138893  0.507519\n","7              moheia_vest  0.011385 NaN  0.138893  0.507519\n","8                elgeseter  0.011385 NaN  0.138893  0.507519\n","9                e6_tiller  0.011385 NaN  0.138893  0.507519\n","10        hansjordnesbukta  0.011385 NaN  0.138893  0.507519\n","11                 solheim  0.011385 NaN  0.138893  0.507519\n","12                furulund  0.011385 NaN  0.138893  0.507519\n","13      rv_4__aker_sykehus  0.011385 NaN  0.138893  0.507519\n","14                  torvet  0.011385 NaN  0.138893  0.507519\n","15                hjortnes  0.011385 NaN  0.138893  0.507519\n","16          lensmannsdalen  0.011385 NaN  0.138893  0.507519\n","17                   alvim  0.011385 NaN  0.138893  0.507519\n","18           schancheholen  0.011385 NaN  0.138893  0.507519\n","19             bankplassen  0.011385 NaN  0.138893  0.507519\n","20             bakke_kirke  0.011385 NaN  0.138893  0.507519\n","21          ringsakervegen  0.011385 NaN  0.138893  0.507519\n","22          eilif_dues_vei  0.011385 NaN  0.138893  0.507519\n","23   lillehammer_barnehage  0.011385 NaN  0.138893  0.507519\n","24               bekkestua  0.011385 NaN  0.138893  0.507519\n","25                st_croix  0.011385 NaN  0.138893  0.507519\n","26         rolland__aÃäsane  0.011385 NaN  0.138893  0.507519\n","27  minnesundvegen__gj√∏vik  0.011385 NaN  0.138893  0.507519\n","28                vigernes  0.011385 NaN  0.138893  0.507519\n","29          nedre_langgate  0.011385 NaN  0.138893  0.507519\n","30              kirkeveien  0.011385 NaN  0.138893  0.507519\n","31              loddefjord  0.011385 NaN  0.138893  0.507519\n","32          seljestad_rv83  0.011385 NaN  0.138893  0.507519\n","33          danmarks_plass  0.011385 NaN  0.138893  0.507519\n","34                 vaÃäland  0.011385 NaN  0.138893  0.507519\n","35                  leiret  0.011385 NaN  0.138893  0.507519\n","36        sofienbergparken  0.011385 NaN  0.138893  0.507519\n","37                 smestad  0.011385 NaN  0.138893  0.507519\n","38             bygd√∏y_alle  0.011385 NaN  0.138893  0.507519\n","39          e6_alna_senter  0.011385 NaN  0.138893  0.507519\n","40         knarrdalstranda  0.011385 NaN  0.138893  0.507519\n","41            nygaardsgata  0.011385 NaN  0.138893  0.507519\n","42               manglerud  0.011385 NaN  0.138893  0.507519\n","43             olav_v_gate  0.011385 NaN  0.138893  0.507519"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"GhReeIJLpFNI","executionInfo":{"status":"ok","timestamp":1602882806665,"user_tz":-120,"elapsed":5343,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"17621156606987172084"}},"outputId":"eccb6988-b0cb-4c7a-9f19-57866c7b31dc","colab":{"base_uri":"https://localhost:8080/","height":788}},"source":["# Print years and number of features\n","template = \"{:20}\\t{:>10}\\t{}\"\n","print(template.format(\"Location\", \"# columns\", \"Years\"))\n","for location, data in dataframes.items():\n","    dfs, years = data\n","    years = ', '.join(sorted(years))\n","\n","     # Count number of unique columns\n","    location_columns = set()\n","    for df in dfs:\n","        location_columns.update(df.columns)\n","    num_of_columns = len(location_columns)\n","    print(template.format(location, num_of_columns, years))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Location            \t # columns\tYears\n","E6-Tiller           \t        45\t2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Alvim               \t        41\t2015, 2016, 2017, 2018, 2019, 2020\n","Sofienbergparken    \t        58\t2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Manglerud           \t       104\t2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Leiret              \t        54\t2016, 2017, 2018, 2019, 2020\n","Minnesundvegen, Gj√∏vik\t        42\t2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Vangsveien, Hamar   \t        43\t2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Rv 4, Aker sykehus  \t       106\t2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Ringsakervegen      \t        30\t2018, 2019, 2020\n","Bakke kirke         \t        50\t2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Moheia Vest         \t        17\t2018, 2019, 2020\n","Torvet              \t        64\t2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Klosterhaugen       \t       125\t2017, 2018, 2019, 2020\n","Hansjordnesbukta    \t        47\t2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Nygaardsgata        \t        47\t2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Hjortnes            \t        41\t2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Schancheholen       \t        90\t2018, 2019, 2020\n","St.Croix            \t        30\t2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Bankplassen         \t        30\t2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Kransen             \t        54\t2011, 2012, 2015, 2016, 2017, 2018, 2019, 2020\n","Solheim             \t        67\t2017, 2018, 2019, 2020\n","Knarrdalstranda     \t        28\t2017, 2018, 2019, 2020\n","Furulund            \t        34\t2016, 2017, 2018, 2019, 2020\n","VaÃäland             \t        74\t2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Bryn skole          \t        91\t2018, 2019, 2020\n","Alnabru             \t        92\t2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Olav V gate         \t        41\t2018, 2019, 2020\n","Loddefjord          \t       121\t2015, 2016, 2017, 2018, 2019, 2020\n","Elgeseter           \t        46\t2000, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Rolland, AÃäsane     \t        58\t2015, 2016, 2017, 2018, 2019, 2020\n","Kannik              \t        92\t1996, 1997, 1999, 2000, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Nedre Langgate      \t        54\t2018, 2019, 2020\n","Vigernes            \t        30\t2015, 2016, 2017, 2018, 2019, 2020\n","Kirkeveien          \t        47\t1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Lensmannsdalen      \t        47\t2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Danmarks plass      \t       120\t2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Bygd√∏y Alle         \t        36\t2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Lillehammer barnehage\t        36\t2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Bekkestua           \t        52\t2016, 2017, 2018, 2019, 2020\n","E6 Alna senter      \t        50\t2017, 2018, 2019, 2020\n","Seljestad Rv83      \t        48\t2018, 2019, 2020\n","Smestad             \t        59\t2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n","Eilif Dues vei      \t        51\t2014, 2015, 2016, 2017, 2018, 2019, 2020\n","RaÃädal              \t        36\t2017, 2018, 2019, 2020\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1pmU3MKkFCFB"},"source":["'''\n","def rename_col(name):\n","    \"\"\"Conversion to make column names compatible with BigQuery.\"\"\"\n","    problematic_chars = ' .,:;{}()='\n","    for c in problematic_chars:\n","        name = name.replace(c, '_')\n","    return name\n","'''\n","\n","import re\n","def rename_col(name):\n","    \"\"\"Conversion to make column names compatible with BigQuery.\"\"\"\n","    name = re.sub(\"(PRA\\.(.+)\\.(.+)\\.(.+))\", r'PRA_\\3__\\4', name)\n","    name = re.sub(\"(MET\\.[a-zA-Z0-9]*:0\\.)\", r'MET_', name)\n","    name = re.sub(r\"(NEA\\..*\\.(NOx|PM2_5|PM10|NO2|PM1|NO))\", r'NEA_\\2', name)\n","    name = re.sub(r\"(Kystverket\\.[a-zA-Z0-9_-]*\\.(stationary|moving))\", r'KV_\\2', name)\n","    return name\n","\n","def rename_file(name):\n","    name = name.lower()\n","    for c in \" ,.-:;{}[]()=\":\n","        name = name.replace(c, '_')\n","    return name"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dH681J9hr0JB"},"source":["base_dfs = []\n","\n","for location, lists in dataframes.items():\n","    dfs, years = lists\n","    dfs = list(dfs)\n","    # Rename all columns (Thanks telenor :))\n","\n","    for i, df in enumerate(dfs):\n","        translation = {col: rename_col(col) for col in df.columns}\n","        dfs[i] = df.rename(columns=translation)\n","\n","    # Create new root dataframe for directory\n","    base_df = pd.DataFrame()\n","\n","    # Add location and years data to dataframe\n","    base_df['year'] = None\n","    \n","    # Identify all unique columns\n","    location_columns = set()\n","    for df in dfs:\n","        location_columns.update(df.columns)\n","    \n","    # Generate colummns in base dataframe\n","    for column_name in location_columns:\n","        base_df[column_name] = None\n","\n","    # Add all rows of data to base dataframe\n","    for df, year in zip(dfs, years):\n","        df['year'] = year\n","        base_df = base_df.append(df)\n","    \n","    \n","\n","    # Add location to all rows\n","    base_df.insert(1, 'location', location)\n","    base_df = base_df.reindex(sorted(base_df.columns), axis=1)\n","    base_dfs.append(base_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XS6ID2cgN-uq","executionInfo":{"status":"ok","timestamp":1602882930865,"user_tz":-120,"elapsed":537,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"17621156606987172084"}},"outputId":"654cf956-4d3f-43b2-9391-5e4a7455ef29","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Total number of rows in all datasets\n","import numpy as np\n","print(np.array([df.shape for df in base_dfs]).sum(axis=0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[3700794    2557]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2fRJWsBe9y3e"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SfCwFZHSHqoV"},"source":["summary_dfs = []\n","\n","# Export .parquet files to local storage\n","for location, base_df in zip(dataframes.keys(), base_dfs):\n","    prefixes = [\"MET\", \"KV\", \"PRA\", \"NEA\"]\n","    \n","    location_data = []\n","    col_groups = {prefix: [col for col in df.columns if col.startswith(prefix)] for prefix in prefixes}\n","    for prefix, col_group in col_groups.items():\n","        location_data.append(df[col_group].isnull().sum().sum()/df[col_group].size)\n","    \n","    summary_dfs.append([location] + location_data)\n","\n","pd.DataFrame(summary_dfs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eoeb751R31fs"},"source":["# Upload dataset to Google Cloud Storage\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","FROM = \"/tmp/location_dataset\"\n","TO = \"gs://telenor-data-science/datasets/location_dataset\"\n","IS_DIR = True\n","\n","if IS_DIR:\n","    !gsutil -m cp -r {FROM} {TO}\n","else:\n","    !gsutil cp {FROM} {TO}\n","\n","# Compress and upload compresed version to GCS\n","!tar -czvf /tmp/location_dataset.tar.gz /tmp/location_dataset\n","!gsutil cp /tmp/location_dataset.tar.gz gs://telenor-data-science/datasets/location_dataset.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"srB5DKIalZRZ","executionInfo":{"status":"ok","timestamp":1602889452521,"user_tz":-120,"elapsed":12525,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"17621156606987172084"}},"outputId":"b7e34c15-e303-44bc-a751-736e29bfe381","colab":{"base_uri":"https://localhost:8080/","height":154}},"source":["# DOESNT WORK - Type conflict on parquet inferred schema vs. df index :( \n","#root_df = pd.DataFrame()\n","root_df = base_dfs[0].copy().iloc[0:0]\n","#root_df = root_df.set_index(base_dfs[0].iloc[0:0].index)\n","# Identify all unique columns\n","root_columns = set()\n","for df in base_dfs:\n","    root_columns.update(df.columns)\n","\n","# Generate colummns in root dataframe\n","for column_name in root_columns:\n","    root_df[column_name] = ''\n","\n","# Rearrange columns\n","root_df = root_df.reindex(sorted(root_df.columns), axis=1)\n","\n","#Change col types\n","#root_df.append(base_dfs[0])\n","#root_df = root_df.iloc[0:0]\n","root_df = root_df.astype(float)\n","root_df = root_df.astype({'year': 'str', 'location': 'str'})\n","\n","root_df.to_parquet(f\"/tmp/location_dataset/all_locations.parquet\")\n","!gsutil cp /tmp/location_dataset/all_locations.parquet gs://telenor-data-science/datasets/location_dataset/all_locations.parquet\n","# Create table\n","!bq load --source_format=PARQUET --ignore_unknown_values --autodetect telenor-data-science:location_dataset.all_locations gs://telenor-data-science/datasets/location_dataset/*.parquet "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Copying file:///tmp/location_dataset/all_locations.parquet [Content-Type=application/octet-stream]...\n","/ [0 files][    0.0 B/ 38.7 KiB]                                                \r/ [1 files][ 38.7 KiB/ 38.7 KiB]                                                \r\n","Operation completed over 1 objects/38.7 KiB.                                     \n","Waiting on bqjob_r1b00740624a4021c_0000017533a7e616_1 ... (6s) Current status: DONE   \n","BigQuery error in load operation: Error processing job 'telenor-data-\n","science:bqjob_r1b00740624a4021c_0000017533a7e616_1': Error while reading data,\n","error message: incompatible types for field 'year': INT32 in Parquet vs. string\n","in schema\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x716PjHhma1l","executionInfo":{"status":"ok","timestamp":1602889349471,"user_tz":-120,"elapsed":606,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"17621156606987172084"}},"outputId":"025d7c62-8cfa-421d-f401-7ecc789d8e61","colab":{"base_uri":"https://localhost:8080/","height":223}},"source":["root_df.dtypes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["KV_moving_0m_to_100m          float64\n","KV_moving_10000m_to_30000m    float64\n","KV_moving_1000m_to_3000m      float64\n","KV_moving_100m_to_300m        float64\n","KV_moving_3000m_to_10000m     float64\n","                               ...   \n","PRA_9__from5_6To7_6           float64\n","PRA_9__from7_6To12_5          float64\n","PRA_9__upTo5_6                float64\n","location                       object\n","year                           object\n","Length: 141, dtype: object"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"1_ubtaEbmc5h"},"source":[""],"execution_count":null,"outputs":[]}]}